{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "import gc\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c77d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cached memory using the cached allocator\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b3dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/parallax/env38/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb', apply_api_compatibility=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a69777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\nfrom IPython.display import HTML\\nfrom IPython import display as ipythondisplay\\nimport glob\\nimport io\\nimport base64\\n\\nfrom pyvirtualdisplay import Display\\ndisplay = Display(visible=0, size=(1400, 900))\\ndisplay.start()\\n\\n\"\"\"\\nUtility functions to enable video recording of gym environment and displaying it\\nTo enable video, just do \"env = wrap_env(env)\"\"\\n\"\"\"\\n\\ndef show_video():\\n  mp4list = glob.glob(\\'video/*.mp4\\')\\n  if len(mp4list) > 0:\\n    mp4 = mp4list[0]\\n    video = io.open(mp4, \\'r+b\\').read()\\n    encoded = base64.b64encode(video)\\n    ipythondisplay.display(HTML(data=.format(encoded.decode(\\'ascii\\'))))\\n  else: \\n    print(\"Could not find video\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7583c60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "#   Limit the action-space to \n",
    "# 0. walk right\n",
    "# 1. jump right\n",
    "\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action = 0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa86f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda device available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'Cuda device available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33830454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            #    Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "    \n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype=np.uint8)\n",
    "        \n",
    "    def permute_orientation(self, observation):\n",
    "        # Permute [H,W,C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype = torch.float)\n",
    "        return observation\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "    \n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "        \n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape=obs_shape, dtype = np.uint8)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0,255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e74cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Apply Wrappers to Environment\n",
    "env = SkipFrame(env, skip = 4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape = 84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8556050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b1f26",
   "metadata": {},
   "source": [
    "# Act\n",
    "\n",
    "An action consists of exploit and explore\n",
    "\n",
    "When exploit is used, the MarioNet is used to provide the optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1532b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        \n",
    "        self.state_dim  = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir   = save_dir\n",
    "        self.use_cuda   = torch.cuda.is_available()\n",
    "        self.device = \"cuda\" if self.use_cuda else \"cpu\"\n",
    "        \n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device = self.device)\n",
    "        \n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "        \n",
    "        self.save_every = 5e5 # Number of experiences before saving Mario Net\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        action_idx (int): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        \n",
    "        #    Explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        \n",
    "        \n",
    "        #    Exploit\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device = self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model = \"online\")\n",
    "            action_idx = torch.argmax(action_values, axis = 1).item()\n",
    "            \n",
    "        # decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_date  = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa738d",
   "metadata": {},
   "source": [
    "# Cache and Recall\n",
    "\n",
    "These two methods function as marios \"memory\"\n",
    "\n",
    "## cache\n",
    "For each action, the experience is stores in memory. The experience includes: current state, action performed, reward for the action, the next state and is the game is done\n",
    "\n",
    "## recall\n",
    "Randomly sample a batch of experiences from Marios memory to learn the game \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e73b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward])\n",
    "        done   = torch.BoolTensor([done]).cuda()\n",
    "    \n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "        \n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of memories from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            state, next_state, action, reward, done = state.cuda(), next_state.cuda(), action.cuda(), reward.cuda(), done.cuda()\n",
    "        \n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f7df0",
   "metadata": {},
   "source": [
    "# Learn\n",
    "\n",
    "DDQN algorithmn\n",
    "[q_learning](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "Uses two networks Qonline and Qtarget that independently approximate optimal action-value \n",
    "\n",
    "We share a feature generator `features` across Qonline and Qtarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d01c431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"\n",
    "    input -> (conv+relu)*3 -> flatten -> (dense + relu) X 2 -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "            \n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = c, out_channels = 32, kernel_size = 8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "        \n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        \n",
    "        # Q_target params are frozen\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee65b26",
   "metadata": {},
   "source": [
    "# TD Estimate and Target\n",
    "\n",
    "TD Estimate - the predicted Q* for a given state \n",
    "\n",
    "TD Target - Aggregation of current reward and estimated Q* in the next state s'\n",
    "\n",
    "@torch.no_grad() halts backpropagation on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de46e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        '''\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ].astype(float)  # Q_online(s,a)\n",
    "        '''\n",
    "        curr_state_q = self.net(state, model='online')\n",
    "        current_Q =  curr_state_q[np.arange(0, self.batch_size), action]\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"target\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        '''\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        '''\n",
    "        next_state_q = self.net(next_state, model = 'target')\n",
    "        next_Q = next_state_q[np.arange(0, self.batch_size), action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec0afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ed16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f248f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0590f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce4d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 48 - Epsilon 0.9999880000704982 - Mean Reward 232.0 - Mean Length 48.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.47 - Time 2023-01-16T19:26:19\n",
      "Episode 20 - Step 7208 - Epsilon 0.9981996224020983 - Mean Reward 720.333 - Mean Length 343.238 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 72.537 - Time 2023-01-16T19:27:32\n",
      "Episode 40 - Step 13775 - Epsilon 0.9965621724766063 - Mean Reward 672.829 - Mean Length 335.976 - Mean Loss 0.12 - Mean Q Value 0.406 - Time Delta 99.44 - Time 2023-01-16T19:29:11\n",
      "Episode 60 - Step 21620 - Epsilon 0.9946095800583177 - Mean Reward 700.377 - Mean Length 354.426 - Mean Loss 0.18 - Mean Q Value 1.059 - Time Delta 255.952 - Time 2023-01-16T19:33:27\n",
      "Episode 80 - Step 27386 - Epsilon 0.9931768830338418 - Mean Reward 685.074 - Mean Length 338.099 - Mean Loss 0.206 - Mean Q Value 1.735 - Time Delta 211.903 - Time 2023-01-16T19:36:59\n",
      "Episode 100 - Step 34587 - Epsilon 0.9913905245549917 - Mean Reward 705.59 - Mean Length 345.39 - Mean Loss 0.225 - Mean Q Value 2.378 - Time Delta 166.554 - Time 2023-01-16T19:39:45\n",
      "Episode 120 - Step 42593 - Epsilon 0.9894082406044394 - Mean Reward 718.27 - Mean Length 353.85 - Mean Loss 0.282 - Mean Q Value 3.582 - Time Delta 141.241 - Time 2023-01-16T19:42:07\n",
      "Episode 140 - Step 49971 - Epsilon 0.9875849589149491 - Mean Reward 744.48 - Mean Length 361.96 - Mean Loss 0.293 - Mean Q Value 4.818 - Time Delta 153.285 - Time 2023-01-16T19:44:40\n",
      "Episode 160 - Step 57358 - Epsilon 0.9857628192001733 - Mean Reward 726.92 - Mean Length 357.38 - Mean Loss 0.313 - Mean Q Value 5.997 - Time Delta 159.208 - Time 2023-01-16T19:47:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31648/2888092854.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done   = torch.BoolTensor([done]).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180 - Step 65302 - Epsilon 0.9838070367313226 - Mean Reward 766.51 - Mean Length 379.16 - Mean Loss 0.335 - Mean Q Value 7.001 - Time Delta 142.725 - Time 2023-01-16T19:49:42\n",
      "Episode 200 - Step 72130 - Epsilon 0.9821291104271858 - Mean Reward 763.47 - Mean Length 375.43 - Mean Loss 0.361 - Mean Q Value 7.967 - Time Delta 117.009 - Time 2023-01-16T19:51:39\n",
      "Episode 220 - Step 77936 - Epsilon 0.9807045839453351 - Mean Reward 730.68 - Mean Length 353.43 - Mean Loss 0.398 - Mean Q Value 9.032 - Time Delta 97.766 - Time 2023-01-16T19:53:17\n",
      "Episode 240 - Step 84180 - Epsilon 0.9791748981287696 - Mean Reward 710.97 - Mean Length 342.09 - Mean Loss 0.439 - Mean Q Value 10.054 - Time Delta 106.653 - Time 2023-01-16T19:55:03\n",
      "Episode 260 - Step 91569 - Epsilon 0.9773677866792466 - Mean Reward 701.8 - Mean Length 342.11 - Mean Loss 0.46 - Mean Q Value 10.877 - Time Delta 127.841 - Time 2023-01-16T19:57:11\n",
      "Episode 280 - Step 98585 - Epsilon 0.9756549859324496 - Mean Reward 677.14 - Mean Length 332.83 - Mean Loss 0.477 - Mean Q Value 11.538 - Time Delta 120.338 - Time 2023-01-16T19:59:11\n",
      "Episode 300 - Step 104478 - Epsilon 0.9742186603378176 - Mean Reward 674.36 - Mean Length 323.48 - Mean Loss 0.501 - Mean Q Value 12.173 - Time Delta 101.304 - Time 2023-01-16T20:00:53\n",
      "Episode 320 - Step 113515 - Epsilon 0.9720201409918865 - Mean Reward 715.11 - Mean Length 355.79 - Mean Loss 0.514 - Mean Q Value 12.756 - Time Delta 182.789 - Time 2023-01-16T20:03:56\n",
      "Episode 340 - Step 120695 - Epsilon 0.9702769296209249 - Mean Reward 718.42 - Mean Length 365.15 - Mean Loss 0.517 - Mean Q Value 13.188 - Time Delta 130.031 - Time 2023-01-16T20:06:06\n",
      "Episode 360 - Step 127511 - Epsilon 0.9686249853823329 - Mean Reward 725.46 - Mean Length 359.42 - Mean Loss 0.518 - Mean Q Value 13.473 - Time Delta 117.81 - Time 2023-01-16T20:08:03\n",
      "Episode 380 - Step 134857 - Epsilon 0.9668477378313516 - Mean Reward 698.2 - Mean Length 362.72 - Mean Loss 0.533 - Mean Q Value 13.84 - Time Delta 126.571 - Time 2023-01-16T20:10:10\n",
      "Episode 400 - Step 141674 - Epsilon 0.9652013906601887 - Mean Reward 719.68 - Mean Length 371.96 - Mean Loss 0.536 - Mean Q Value 14.089 - Time Delta 116.424 - Time 2023-01-16T20:12:06\n",
      "Episode 420 - Step 146545 - Epsilon 0.9640267318852374 - Mean Reward 676.28 - Mean Length 330.3 - Mean Loss 0.548 - Mean Q Value 14.449 - Time Delta 80.032 - Time 2023-01-16T20:13:26\n",
      "Episode 440 - Step 151877 - Epsilon 0.9627425401948474 - Mean Reward 668.67 - Mean Length 311.82 - Mean Loss 0.575 - Mean Q Value 15.037 - Time Delta 89.966 - Time 2023-01-16T20:14:56\n",
      "Episode 460 - Step 157450 - Epsilon 0.9614021329609983 - Mean Reward 672.12 - Mean Length 299.39 - Mean Loss 0.622 - Mean Q Value 16.104 - Time Delta 94.08 - Time 2023-01-16T20:16:30\n",
      "Episode 480 - Step 163784 - Epsilon 0.959880957201863 - Mean Reward 710.74 - Mean Length 289.27 - Mean Loss 0.678 - Mean Q Value 17.354 - Time Delta 108.07 - Time 2023-01-16T20:18:19\n",
      "Episode 500 - Step 170322 - Epsilon 0.9583133130872973 - Mean Reward 700.81 - Mean Length 286.48 - Mean Loss 0.722 - Mean Q Value 18.467 - Time Delta 113.056 - Time 2023-01-16T20:20:12\n",
      "Episode 520 - Step 178070 - Epsilon 0.9564588565887236 - Mean Reward 709.12 - Mean Length 315.25 - Mean Loss 0.736 - Mean Q Value 19.197 - Time Delta 134.187 - Time 2023-01-16T20:22:26\n",
      "Episode 540 - Step 186164 - Epsilon 0.9545254186659987 - Mean Reward 750.75 - Mean Length 342.87 - Mean Loss 0.73 - Mean Q Value 19.404 - Time Delta 140.986 - Time 2023-01-16T20:24:47\n",
      "Episode 560 - Step 193022 - Epsilon 0.9528902867484538 - Mean Reward 764.28 - Mean Length 355.72 - Mean Loss 0.71 - Mean Q Value 19.013 - Time Delta 118.682 - Time 2023-01-16T20:26:45\n",
      "Episode 580 - Step 199678 - Epsilon 0.9513059956088383 - Mean Reward 757.6 - Mean Length 358.94 - Mean Loss 0.681 - Mean Q Value 18.609 - Time Delta 118.413 - Time 2023-01-16T20:28:44\n",
      "Episode 600 - Step 205490 - Epsilon 0.9499247515411903 - Mean Reward 737.65 - Mean Length 351.68 - Mean Loss 0.667 - Mean Q Value 18.661 - Time Delta 98.022 - Time 2023-01-16T20:30:22\n",
      "Episode 620 - Step 210556 - Epsilon 0.9487224332216934 - Mean Reward 741.94 - Mean Length 324.86 - Mean Loss 0.683 - Mean Q Value 18.924 - Time Delta 83.696 - Time 2023-01-16T20:31:46\n",
      "Episode 640 - Step 217109 - Epsilon 0.9471694609268869 - Mean Reward 725.69 - Mean Length 309.45 - Mean Loss 0.7 - Mean Q Value 19.785 - Time Delta 111.968 - Time 2023-01-16T20:33:38\n",
      "Episode 660 - Step 223113 - Epsilon 0.9457488258416157 - Mean Reward 733.54 - Mean Length 300.91 - Mean Loss 0.717 - Mean Q Value 20.807 - Time Delta 103.241 - Time 2023-01-16T20:35:21\n",
      "Episode 680 - Step 229693 - Epsilon 0.9441943477373531 - Mean Reward 734.75 - Mean Length 300.15 - Mean Loss 0.736 - Mean Q Value 21.686 - Time Delta 132.331 - Time 2023-01-16T20:37:33\n",
      "Episode 700 - Step 237150 - Epsilon 0.9424357729253837 - Mean Reward 758.56 - Mean Length 316.6 - Mean Loss 0.743 - Mean Q Value 22.088 - Time Delta 149.873 - Time 2023-01-16T20:40:03\n",
      "Episode 720 - Step 244614 - Epsilon 0.9406788272933204 - Mean Reward 776.68 - Mean Length 340.58 - Mean Loss 0.743 - Mean Q Value 21.942 - Time Delta 129.582 - Time 2023-01-16T20:42:13\n",
      "Episode 740 - Step 250144 - Epsilon 0.9393792372006028 - Mean Reward 767.72 - Mean Length 330.35 - Mean Loss 0.759 - Mean Q Value 21.62 - Time Delta 93.683 - Time 2023-01-16T20:43:46\n",
      "Episode 760 - Step 256677 - Epsilon 0.9378462480876144 - Mean Reward 736.42 - Mean Length 335.64 - Mean Loss 0.786 - Mean Q Value 21.304 - Time Delta 113.231 - Time 2023-01-16T20:45:40\n",
      "Episode 780 - Step 265048 - Epsilon 0.935885622371871 - Mean Reward 741.92 - Mean Length 353.55 - Mean Loss 0.792 - Mean Q Value 20.843 - Time Delta 147.308 - Time 2023-01-16T20:48:07\n",
      "Episode 800 - Step 271607 - Epsilon 0.9343522612387349 - Mean Reward 721.26 - Mean Length 344.57 - Mean Loss 0.787 - Mean Q Value 20.114 - Time Delta 147.693 - Time 2023-01-16T20:50:35\n",
      "Episode 820 - Step 278882 - Epsilon 0.9326544522641901 - Mean Reward 735.56 - Mean Length 342.68 - Mean Loss 0.765 - Mean Q Value 19.922 - Time Delta 149.393 - Time 2023-01-16T20:53:04\n",
      "Episode 840 - Step 285041 - Epsilon 0.931219502407268 - Mean Reward 744.06 - Mean Length 348.97 - Mean Loss 0.739 - Mean Q Value 20.004 - Time Delta 111.524 - Time 2023-01-16T20:54:55\n",
      "Episode 860 - Step 290511 - Epsilon 0.9299469298982478 - Mean Reward 755.69 - Mean Length 338.34 - Mean Loss 0.696 - Mean Q Value 20.302 - Time Delta 108.745 - Time 2023-01-16T20:56:44\n",
      "Episode 880 - Step 297019 - Epsilon 0.9284351362317529 - Mean Reward 746.52 - Mean Length 319.71 - Mean Loss 0.663 - Mean Q Value 20.651 - Time Delta 112.517 - Time 2023-01-16T20:58:37\n",
      "Episode 900 - Step 302609 - Epsilon 0.927138554164353 - Mean Reward 734.99 - Mean Length 310.02 - Mean Loss 0.659 - Mean Q Value 21.433 - Time Delta 94.17 - Time 2023-01-16T21:00:11\n",
      "Episode 920 - Step 307588 - Epsilon 0.9259852162623211 - Mean Reward 694.09 - Mean Length 287.06 - Mean Loss 0.663 - Mean Q Value 22.19 - Time Delta 93.961 - Time 2023-01-16T21:01:45\n",
      "Episode 940 - Step 313757 - Epsilon 0.924558216062337 - Mean Reward 691.08 - Mean Length 287.16 - Mean Loss 0.68 - Mean Q Value 22.524 - Time Delta 105.674 - Time 2023-01-16T21:03:30\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "episodes = 10000\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "        \n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        #    Clear non essential variables from GPU memory\n",
    "        del q, loss\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
